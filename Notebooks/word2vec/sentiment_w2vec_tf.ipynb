{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  the dataset\n",
    "\n",
    "The text 8 dataset for training the word_2_vector model is taken from (_http://mattmahoney.net/dc/_), if the zip file is missing it is downloaded again. Roughly 200k unique words in the sample of senteces with roughly 17M words. Download can take a while. Training is then done in run_Word2Vec module. Load the interface to the model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is missing, retrieving  http://mattmahoney.net/dc/text8.zip\n",
      "Found and verified text8.zip \n",
      "Words in file  17005207\n"
     ]
    }
   ],
   "source": [
    "import Load_Text_Set as l_data\n",
    "import run_Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train the model\n",
    "Run the training and return the final normalized embeddings. The model can be trained useing skipgram or continuos bag of string CBOW, controlled by a switch in the file _tf_W0rd2Vec.py_. Othe paramaters of the model are also hard coded in here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip \n",
      "Words in file  17005207\n",
      "Initialized embeddings\n",
      "(200000, 128)\n",
      "Initialized\n",
      "Average loss at step 0: 9.255837\n",
      "Nearest to by: chickpea, device, filming, compound, homebase, stergiopoulos, jahrb, durante,\n",
      "Nearest to people: alexandrov, cherno, slipper, phaenomenologica, getaway, kiskil, lushan, ajiva,\n",
      "Nearest to four: karachaganak, kirsan, utility, ngen, mucel, arguers, stilpo, kirundo,\n",
      "Nearest to can: tylos, berdan, gnd, trollish, aumeier, brilliants, specte, lorenzattractor,\n",
      "Nearest to d: pk, enumerate, surr, valkyries, succoth, vakatakas, onesicritus, mirv,\n",
      "Nearest to been: burgundes, mudhol, chene, schemer, fiurenzu, lavey, tarcher, pastries,\n",
      "Nearest to so: hydrocactus, perata, mora, cursing, dkos, lookalikes, diaphragmic, langland,\n",
      "Nearest to while: halite, butkus, zapp, chdir, mascott, inbox, toolchain, grunt,\n",
      "Nearest to seven: sprengel, leavelle, pertain, improvisers, sportbund, friable, ionization, backlighting,\n",
      "Nearest to were: skvirsky, cushioned, lantern, computationally, seychellois, voluntaryism, oleo, yuba,\n",
      "Nearest to UNK: father, webspace, leva, cased, atropat, caldara, ussy, ipsilateral,\n",
      "Nearest to system: socoh, vardy, caimans, phumi, mikel, oilgate, biogenic, prescence,\n",
      "Nearest to have: gelling, andechs, oliwa, kielten, hits, pepelu, vaishali, homebush,\n",
      "Nearest to see: jestem, earnest, edonkey, negreiros, stiggs, roundway, muqtana, vulcain,\n",
      "Nearest to after: gonzo, snorra, reservoirs, udalski, hatounian, hasidim, kocher, gedara,\n",
      "Nearest to five: ciliophora, lateline, bonaj, square, circumvent, elapsed, harrigan, patission,\n",
      "Average loss at step 2000: 4.520401\n",
      "Average loss at step 4000: 3.777777\n",
      "Average loss at step 6000: 3.549556\n",
      "Average loss at step 8000: 3.378672\n",
      "Average loss at step 10000: 3.282246\n",
      "Nearest to by: dragut, with, cooeeing, for, caveats, from, quadrivium, adn,\n",
      "Nearest to people: years, kiskil, crouched, furies, disobey, cordoning, getaway, ideas,\n",
      "Nearest to four: six, eight, seven, five, three, nine, zero, two,\n",
      "Nearest to can: may, would, could, will, should, must, to, makrani,\n",
      "Nearest to d: b, anuran, waive, biometrical, pelee, rula, biopreparat, aerarium,\n",
      "Nearest to been: schemer, be, stejneger, developed, sepia, lavey, info, wyche,\n",
      "Nearest to so: hydrocactus, dkos, ljungby, mora, langland, hortons, cleomedes, hrithik,\n",
      "Nearest to while: jasta, thielmann, mascott, for, judiciously, baffin, yevsektsiya, ariadne,\n",
      "Nearest to seven: eight, six, nine, four, five, three, zero, two,\n",
      "Nearest to were: are, was, have, had, brinsley, ditko, astyanax, tenchian,\n",
      "Nearest to UNK: webspace, leva, cased, atropat, caldara, ussy, ipsilateral, chey,\n",
      "Nearest to system: titanomachy, vardy, ogopogo, caimans, mikel, hlfic, dummar, radiological,\n",
      "Nearest to have: had, has, were, be, are, creature, vaishali, blacklight,\n",
      "Nearest to see: aos, nanotubes, sedates, premade, zezuru, heer, kindergartens, palmiest,\n",
      "Nearest to after: brustein, wiwaxia, from, typographies, snorra, when, hodierna, rossman,\n",
      "Nearest to five: four, six, three, seven, eight, nine, zero, two,\n",
      "Average loss at step 12000: 3.286572\n",
      "Average loss at step 14000: 3.238507\n",
      "Average loss at step 16000: 3.256216\n",
      "Average loss at step 18000: 3.187976\n",
      "Average loss at step 20000: 3.039966\n",
      "Nearest to by: dragut, hpcc, farago, was, hundertwasserhaus, durey, chickpea, spectroscopic,\n",
      "Nearest to people: ideas, furies, crouched, rideau, caradon, correcting, getaway, grignard,\n",
      "Nearest to four: six, three, eight, five, seven, two, nine, zero,\n",
      "Nearest to can: may, would, will, could, must, should, might, cannot,\n",
      "Nearest to d: b, anuran, waive, pelee, terni, neurodevelopmental, peterzano, semantically,\n",
      "Nearest to been: schemer, be, become, was, polytopes, were, bristow, stejneger,\n",
      "Nearest to so: mla, libby, mora, ljungby, datadisk, lifecycle, fox, ishq,\n",
      "Nearest to while: although, though, however, and, but, casamayor, including, ariadne,\n",
      "Nearest to seven: eight, six, nine, five, three, four, two, zero,\n",
      "Nearest to were: are, have, was, sarcocheilichthys, pamper, tenchian, ditko, being,\n",
      "Nearest to UNK: webspace, leva, cased, atropat, caldara, ussy, ipsilateral, chey,\n",
      "Nearest to system: systems, ngm, ely, vardy, strolled, suttee, stratocaster, aramean,\n",
      "Nearest to have: has, had, were, are, be, creature, include, fce,\n",
      "Nearest to see: sedates, belemnite, heer, comunali, but, kindergartens, praesidentiae, palmiest,\n",
      "Nearest to after: before, when, wiwaxia, snorra, brustein, until, usurer, abdank,\n",
      "Nearest to five: six, eight, seven, three, four, nine, two, zero,\n",
      "Average loss at step 22000: 3.114988\n",
      "Average loss at step 24000: 3.071707\n",
      "Average loss at step 26000: 3.043790\n",
      "Average loss at step 28000: 3.051388\n",
      "Average loss at step 30000: 3.025266\n",
      "Nearest to by: durey, exitu, dragut, verein, through, sarit, zmrzl, sagal,\n",
      "Nearest to people: men, children, alcindor, ideas, crouched, codeworks, macheth, furies,\n",
      "Nearest to four: five, six, eight, seven, nine, three, two, zero,\n",
      "Nearest to can: may, could, would, will, must, should, might, cannot,\n",
      "Nearest to d: b, pelee, anuran, waive, neurodevelopmental, ertzaintza, c, yachtsmen,\n",
      "Nearest to been: be, become, schemer, polytopes, were, was, homse, already,\n",
      "Nearest to so: mla, gerund, antoinette, datadisk, less, filmmakers, veel, hinting,\n",
      "Nearest to while: when, although, though, after, however, before, kefauver, coudreau,\n",
      "Nearest to seven: eight, six, nine, four, five, three, zero, two,\n",
      "Nearest to were: are, was, have, had, been, astyanax, narain, informative,\n",
      "Nearest to UNK: webspace, leva, cased, atropat, caldara, ussy, ipsilateral, chey,\n",
      "Nearest to system: systems, situation, ely, titanomachy, stratocaster, strolled, iaw, coca,\n",
      "Nearest to have: had, has, are, were, having, include, webexhibits, iditarod,\n",
      "Nearest to see: but, references, kindergartens, burle, bouzouki, liberalize, sedates, called,\n",
      "Nearest to after: before, when, during, until, while, saw, wiwaxia, within,\n",
      "Nearest to five: four, six, three, eight, seven, nine, zero, two,\n",
      "Average loss at step 32000: 2.846431\n",
      "Average loss at step 34000: 2.979519\n",
      "Average loss at step 36000: 2.965738\n",
      "Average loss at step 38000: 2.957362\n",
      "Average loss at step 40000: 2.976418\n",
      "Nearest to by: when, songfic, through, farago, durey, from, using, in,\n",
      "Nearest to people: men, children, women, alcindor, crouched, tft, systemizes, fasti,\n",
      "Nearest to four: five, eight, seven, three, six, nine, two, zero,\n",
      "Nearest to can: may, could, will, must, would, should, cannot, might,\n",
      "Nearest to d: b, pelee, wikifiction, anuran, waive, fraction, ertzaintza, penzias,\n",
      "Nearest to been: become, be, schemer, polytopes, was, maglev, already, were,\n",
      "Nearest to so: gerund, mla, antoinette, filmmakers, mossadegh, if, sulfuric, veel,\n",
      "Nearest to while: although, though, however, and, when, but, after, or,\n",
      "Nearest to seven: eight, six, five, nine, four, three, zero, two,\n",
      "Nearest to were: are, have, was, had, pamper, include, although, been,\n",
      "Nearest to UNK: webspace, leva, cased, atropat, caldara, ussy, ipsilateral, chey,\n",
      "Nearest to system: systems, device, strolled, situation, bouloustra, appell, axonal, process,\n",
      "Nearest to have: had, has, were, include, are, having, be, webexhibits,\n",
      "Nearest to see: references, but, include, sedates, posavac, burle, heer, noematic,\n",
      "Nearest to after: before, when, during, while, despite, recognitio, through, if,\n",
      "Nearest to five: four, six, seven, eight, three, nine, two, zero,\n",
      "Average loss at step 42000: 2.965305\n",
      "Average loss at step 44000: 2.986973\n",
      "Average loss at step 46000: 2.911373\n",
      "Average loss at step 48000: 2.869899\n",
      "Average loss at step 50000: 2.858816\n",
      "Nearest to by: without, using, through, paco, songfic, quadrivium, hpcc, during,\n",
      "Nearest to people: men, children, women, players, those, individuals, alcindor, students,\n",
      "Nearest to four: five, six, seven, eight, three, nine, two, zero,\n",
      "Nearest to can: may, could, will, must, would, cannot, should, might,\n",
      "Nearest to d: b, anuran, rula, biometrical, pelee, trevino, waive, catecholamines,\n",
      "Nearest to been: become, be, schemer, polytopes, already, was, grown, maglev,\n",
      "Nearest to so: mla, then, gerund, too, alleviating, sometimes, sulfuric, jeremia,\n",
      "Nearest to while: although, though, when, however, but, after, before, if,\n",
      "Nearest to seven: eight, six, nine, four, five, three, zero, two,\n",
      "Nearest to were: are, have, had, was, although, include, been, tonio,\n",
      "Nearest to UNK: webspace, leva, cased, caldara, atropat, ussy, ipsilateral, chey,\n",
      "Nearest to system: systems, situation, process, bidinotto, titanomachy, smollett, device, crisscrossed,\n",
      "Nearest to have: had, has, were, include, having, are, be, provide,\n",
      "Nearest to see: include, references, moammar, but, includes, posavac, laetitia, can,\n",
      "Nearest to after: before, when, during, despite, without, until, while, ralegh,\n",
      "Nearest to five: four, six, seven, three, eight, nine, zero, two,\n",
      "Average loss at step 52000: 2.907825\n",
      "Average loss at step 54000: 2.880096\n",
      "Average loss at step 56000: 2.857745\n",
      "Average loss at step 58000: 2.757673\n",
      "Average loss at step 60000: 2.841038\n",
      "Nearest to by: songfic, tacs, zmrzl, disuade, hpcc, ishshan, gansz, verein,\n",
      "Nearest to people: men, children, women, players, individuals, sherkin, those, pendantes,\n",
      "Nearest to four: five, seven, six, eight, three, nine, two, zero,\n",
      "Nearest to can: could, may, will, would, must, should, might, cannot,\n",
      "Nearest to d: b, ertzaintza, brahmins, maxima, plagued, yachtsmen, supercpu, pelee,\n",
      "Nearest to been: become, be, polytopes, schemer, grown, already, mmixmasters, amidock,\n",
      "Nearest to so: mla, too, gerund, dut, sometimes, antoinette, thus, sulfuric,\n",
      "Nearest to while: although, though, when, however, before, after, csarevich, are,\n",
      "Nearest to seven: eight, six, five, nine, four, three, zero, two,\n",
      "Nearest to were: are, have, had, was, pamper, be, although, arubans,\n",
      "Nearest to UNK: webspace, leva, cased, caldara, atropat, ussy, ipsilateral, chey,\n",
      "Nearest to system: systems, process, device, situation, crisscrossed, macnelly, rebalances, strolled,\n",
      "Nearest to have: had, has, having, were, provide, include, are, be,\n",
      "Nearest to see: references, include, includes, but, siddons, integralist, moammar, winnetou,\n",
      "Nearest to after: before, when, during, despite, without, ralegh, while, within,\n",
      "Nearest to five: four, seven, six, eight, three, nine, zero, two,\n",
      "Average loss at step 62000: 2.853227\n",
      "Average loss at step 64000: 2.795200\n",
      "Average loss at step 66000: 2.759334\n",
      "Average loss at step 68000: 2.724250\n",
      "Average loss at step 70000: 2.846458\n",
      "Nearest to by: durey, without, industrialists, caveats, disuade, paco, zmrzl, songfic,\n",
      "Nearest to people: women, men, children, players, authors, individuals, words, those,\n",
      "Nearest to four: three, five, seven, six, eight, two, nine, zero,\n",
      "Nearest to can: could, will, must, may, would, should, might, cannot,\n",
      "Nearest to d: b, brahmins, pumi, dmort, biometrical, biopreparat, bickler, wights,\n",
      "Nearest to been: become, be, already, were, schemer, grown, was, eschnapur,\n",
      "Nearest to so: mla, too, sometimes, gerund, antoinette, very, mathematik, prevalance,\n",
      "Nearest to while: although, when, though, before, however, after, during, but,\n",
      "Nearest to seven: five, eight, six, nine, three, four, zero, two,\n",
      "Nearest to were: are, have, was, including, had, pamper, include, been,\n",
      "Nearest to UNK: tautomer, cased, laxmanniaceae, caldara, lavaur, girdler, cedille, ussy,\n",
      "Nearest to system: systems, process, device, macnelly, project, strolled, archein, crisscrossed,\n",
      "Nearest to have: had, has, include, are, were, having, provide, be,\n",
      "Nearest to see: references, includes, kindergartens, include, moammar, integralist, handhelds, rapida,\n",
      "Nearest to after: before, during, when, despite, while, without, if, from,\n",
      "Nearest to five: six, seven, eight, four, nine, three, zero, two,\n",
      "Average loss at step 72000: 2.800327\n",
      "Average loss at step 74000: 2.632470\n",
      "Average loss at step 76000: 2.805637\n",
      "Average loss at step 78000: 2.834397\n",
      "Average loss at step 80000: 2.784232\n",
      "Nearest to by: without, using, during, hpcc, durey, tacs, reexported, gansz,\n",
      "Nearest to people: children, women, men, individuals, jews, scholars, authors, persons,\n",
      "Nearest to four: five, seven, three, six, eight, nine, two, zero,\n",
      "Nearest to can: could, must, may, will, would, should, might, cannot,\n",
      "Nearest to d: b, yachtsmen, pelee, swindlers, denotational, maxima, moreever, brahmins,\n",
      "Nearest to been: become, be, schemer, polytopes, was, grown, malophoros, already,\n",
      "Nearest to so: too, mla, sometimes, alexi, very, varnishes, franchot, gerund,\n",
      "Nearest to while: although, though, when, before, after, where, however, csarevich,\n",
      "Nearest to seven: six, eight, five, nine, four, three, two, zero,\n",
      "Nearest to were: are, have, was, had, including, those, include, vai,\n",
      "Nearest to UNK: vallens, lect, handwraps, hissariik, centralising, ovate, millefiori, fleming,\n",
      "Nearest to system: systems, process, device, macnelly, archein, situation, crisscrossed, waserfl,\n",
      "Nearest to have: had, has, include, were, are, refer, provide, produce,\n",
      "Nearest to see: include, includes, references, integralist, but, kindergartens, laetitia, vialli,\n",
      "Nearest to after: before, during, despite, when, without, while, ralegh, contempl,\n",
      "Nearest to five: six, seven, eight, four, three, nine, two, zero,\n",
      "Average loss at step 82000: 2.659964\n",
      "Average loss at step 84000: 2.757100\n",
      "Average loss at step 86000: 2.728796\n",
      "Average loss at step 88000: 2.746146\n",
      "Average loss at step 90000: 2.727805\n",
      "Nearest to by: through, using, including, sarit, zmrzl, durey, maliki, gaillard,\n",
      "Nearest to people: women, men, children, individuals, players, authors, users, jews,\n",
      "Nearest to four: six, five, eight, seven, three, nine, two, zero,\n",
      "Nearest to can: could, would, must, should, will, might, may, cannot,\n",
      "Nearest to d: b, pelee, yachtsmen, brahmins, swindlers, ceramicists, legio, denotational,\n",
      "Nearest to been: become, be, were, schemer, grown, polytopes, undergone, mmixmasters,\n",
      "Nearest to so: too, mla, antoinette, sometimes, gerund, zeners, alleviating, felsite,\n",
      "Nearest to while: although, though, when, before, including, csarevich, but, however,\n",
      "Nearest to seven: eight, six, nine, five, four, three, zero, two,\n",
      "Nearest to were: are, have, had, those, was, been, although, arubans,\n",
      "Nearest to UNK: amanah, l, te, catalan, shikar, darlene, caesius, san,\n",
      "Nearest to system: systems, process, macnelly, archein, device, chonas, bidinotto, donors,\n",
      "Nearest to have: had, has, are, were, include, having, provide, be,\n",
      "Nearest to see: include, includes, laetitia, but, references, integralist, kindergartens, external,\n",
      "Nearest to after: before, during, despite, when, without, ralegh, wiwaxia, recognitio,\n",
      "Nearest to five: four, six, seven, eight, three, nine, zero, two,\n",
      "Average loss at step 92000: 2.662648\n",
      "Average loss at step 94000: 2.718229\n",
      "Average loss at step 96000: 2.693342\n",
      "Average loss at step 98000: 2.368214\n",
      "Average loss at step 100000: 2.399122\n",
      "Nearest to by: through, without, with, from, zmrzl, durey, industrialists, when,\n",
      "Nearest to people: men, women, children, individuals, authors, persons, citizens, players,\n",
      "Nearest to four: six, five, seven, three, eight, nine, zero, two,\n",
      "Nearest to can: could, must, might, will, cannot, should, would, may,\n",
      "Nearest to d: b, posies, rula, dmort, kretschmer, anuran, lactis, blaisdell,\n",
      "Nearest to been: become, be, already, were, schemer, undergone, remained, grown,\n",
      "Nearest to so: too, mla, then, sometimes, therefore, very, felsite, thus,\n",
      "Nearest to while: although, when, though, before, after, but, however, where,\n",
      "Nearest to seven: eight, six, four, five, nine, three, zero, two,\n",
      "Nearest to were: are, was, have, include, had, been, be, is,\n",
      "Nearest to UNK: hicom, classic, tejaswini, catalan, inadequate, keesville, escudos, usud,\n",
      "Nearest to system: systems, process, device, macnelly, archein, chonas, bidinotto, crisscrossed,\n",
      "Nearest to have: had, has, include, having, refer, are, were, contain,\n",
      "Nearest to see: include, laetitia, includes, references, prays, vinca, inoxia, called,\n",
      "Nearest to after: before, when, without, despite, during, while, ralegh, for,\n",
      "Nearest to five: six, four, seven, eight, three, nine, zero, two,\n"
     ]
    }
   ],
   "source": [
    "words = l_data.text_8(200000)\n",
    "embeddings = w2v.run_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Some crude attempts at sentiment analysis\n",
    "As part of the project we would like to associate the mood to a memory by assigning an value between 0 and 1 to a series of different moods:\n",
    "\n",
    "* This study incleds 5 moods were identified: _joy_, _sad_, _fear_, _disgust_, _scary_ and _anger_.\n",
    "\n",
    "* For each mood, a set of synonymns are used and the score is taken as the synonymn and sentece average cosine distance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "joy_words = ['happy','joy','pleasure','glee']\n",
    "sad_words = ['sad','unhappy','gloomy']\n",
    "scary_words = ['scary','frightening','terrifying', 'horrifying']\n",
    "disgust_words = ['disgust', 'distaste', 'repulsion']\n",
    "anger_words = ['anger','rage','irritated']\n",
    "\n",
    "def syn_average(word, list_words = []):\n",
    "    to_ret = 0\n",
    "    count = 0 #use this in case a word isnt in dict\n",
    "    for syn in list_words:\n",
    "        if syn in words.dictionary:\n",
    "            syn_id = words.dictionary[syn]\n",
    "            to_ret+=np.matmul(embeddings[word].reshape(1,128), embeddings[syn_id].reshape(128,1))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(syn,\" is not in dict\")\n",
    "    return to_ret/count\n",
    "\n",
    "def test(string_words):\n",
    "    happy = words.dictionary['joy']\n",
    "    sad = words.dictionary['fear']\n",
    "    scary = words.dictionary['sad']\n",
    "    disgust = words.dictionary['disgust']\n",
    "    anger = words.dictionary['anger']\n",
    "    \n",
    "    \n",
    "    d2happy = 0 \n",
    "    d2sad = 0 \n",
    "    d2scary = 0 \n",
    "    d2disgust = 0\n",
    "    d2anger = 0\n",
    "    for a in string_words:\n",
    "        if a in words.dictionary:\n",
    "            in_dict = words.dictionary[a]\n",
    "            d2happy += syn_average(in_dict,joy_words)\n",
    "            d2sad += syn_average(in_dict,sad_words)\n",
    "            d2scary += syn_average(in_dict,scary_words)\n",
    "            d2disgust += syn_average(in_dict,disgust_words)\n",
    "            d2anger += syn_average(in_dict,anger_words )\n",
    "            \n",
    "    d2happy = d2happy/len(string_words)\n",
    "    d2sad = d2sad/len(string_words)\n",
    "    d2scary = d2scary/len(string_words)\n",
    "    d2disgust = d2disgust/len(string_words)\n",
    "    d2anger = d2anger/len(string_words)\n",
    "    print(  max(d2happy,0),\"\\t\",max(d2sad,0),\"\\t\", max(d2scary,0),\"\\t\", max(d2disgust,0),\"\\t\", max(d2anger,0))\n",
    "\n",
    "def plot_emotions(top = 8):\n",
    "    emotions= [ words.dictionary['joy'], words.dictionary['fear'],\n",
    "        words.dictionary['sad'], words.dictionary['disgust'], words.dictionary['anger'] ]\n",
    "        \n",
    "    for i,i_word in enumerate(emotions):\n",
    "        sim = embeddings.similarity(embeddings)        \n",
    "        nearest = (-sim[i_word, :]).argsort()[1:top+1]\n",
    "        print('Nearest to ', emotions[i], \": \")\n",
    "        for k in range(top):\n",
    "            close_word = words.reverse_dictionary(nearest[k])\n",
    "            print('\\t',close_word)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Proof of principal - _ish_\n",
    "\n",
    "To test that the algorithms are worlking we run over a couple of sentences, identified by a human to be happy, scary and agry respectively.\n",
    "\n",
    "Negative scores (i.e. the sentences average embedding vectors points in opposite direction relative to the mood) are set to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sentence: \n",
      "Even Harry, who knew nothing about the different brooms, thought it looked wonderful. Sleek and shiny, with a mahogany handle, it had a long tail of neat, straight twigs and Nimbus Two Thousand written in gold near the top. As seven o'clock drew nearer, Harry left the castle and set off in the dusk toward the Quidditch field. Held never been inside the stadium before. Hundreds of seats were raised in stands around the field so that the spectators were high enough to see what was going on. At either end of the field were three golden poles with hoops on the end. They reminded Harry of the little plastic sticks Muggle children blew bubbles through, except that they were fifty feet high. Too eager to fly again to wait for Wood, Harry mounted his broomstick and kicked off from the ground. What a feeling -- he swooped in and out of the goal posts and then sped up and down the field. The Nimbus Two Thousand turned wherever he wanted at his lightest touch.\n",
      "Similarity to: \n",
      "happy \t\t sad \t\t scary \t\t disgust \t\t anger\n",
      "[[ 0.00192716]] \t [[ 0.01867508]] \t 0 \t [[ 0.00227247]] \t [[ 0.01164252]]\n",
      "\n",
      "\n",
      "Sentence: \n",
      "and the next second, Harry felt Quirrell's hand close on his wrist. At once, a needle-sharp pain seared across Harry's scar; his head felt as though it was about to split in two; he yelled, struggling with all his might, and to his surprise, Quirrell let go of him. The pain in his head lessened -- he looked around wildly to see where Quirrell had gone, and saw him hunched in pain, looking at his fingers -- they were blistering before his eyes.\n",
      "Similarity to: \n",
      "happy \t\t sad \t\t scary \t\t disgust \t\t anger\n",
      "[[ 0.00046383]] \t [[ 0.0208657]] \t 0 \t [[ 0.00422002]] \t [[ 0.0167667]]\n",
      "\n",
      "\n",
      "Sentence: \n",
      "He’d forgotten all about the people in cloaks until he passed a group of them next to the baker’s. He eyed them angrily as he passed. He didn’t know why, but they made him uneasy. This bunch were whispering  excitedly, too, and he couldn’t see a single collectingtin. It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying.\n",
      "Similarity to: \n",
      "happy \t\t sad \t\t scary \t\t disgust \t\t anger\n",
      "0 \t [[ 0.01294638]] \t 0 \t [[ 0.01072488]] \t [[ 0.02178478]]\n"
     ]
    }
   ],
   "source": [
    "happy_string_ = \"Even Harry, who knew nothing about the different brooms, thought it looked wonderful. Sleek and shiny, with a mahogany handle, it had a long tail of neat, straight twigs and Nimbus Two Thousand written in gold near the top. As seven o'clock drew nearer, Harry left the castle and set off in the dusk toward the Quidditch field. Held never been inside the stadium before. Hundreds of seats were raised in stands around the field so that the spectators were high enough to see what was going on. At either end of the field were three golden poles with hoops on the end. They reminded Harry of the little plastic sticks Muggle children blew bubbles through, except that they were fifty feet high. Too eager to fly again to wait for Wood, Harry mounted his broomstick and kicked off from the ground. What a feeling -- he swooped in and out of the goal posts and then sped up and down the field. The Nimbus Two Thousand turned wherever he wanted at his lightest touch.\"\n",
    "scary_string = \"and the next second, Harry felt Quirrell's hand close on his wrist. At once, a needle-sharp pain seared across Harry's scar; his head felt as though it was about to split in two; he yelled, struggling with all his might, and to his surprise, Quirrell let go of him. The pain in his head lessened -- he looked around wildly to see where Quirrell had gone, and saw him hunched in pain, looking at his fingers -- they were blistering before his eyes.\"\n",
    "angry_string = 'He’d forgotten all about the people in cloaks until he passed a group of them next to the baker’s. He eyed them angrily as he passed. He didn’t know why, but they made him uneasy. This bunch were whispering  excitedly, too, and he couldn’t see a single collectingtin. It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying.'\n",
    "\n",
    "happy_string_words = re.sub(r\"\\p{P}+\", \"\", happy_string_).split()\n",
    "scary_string_words = re.sub(r\"\\p{P}+\", \"\", scary_string).split()\n",
    "angry_string_words = re.sub(r\"\\p{P}+\", \"\",angry_string).split()\n",
    "print(\"\\n\")\n",
    "print(\"Sentence: \")\n",
    "print(happy_string_)\n",
    "print(\"Similarity to: \")\n",
    "print(\"happy \\t\\t sad \\t\\t scary \\t\\t disgust \\t\\t anger\")\n",
    "test(happy_string_words)\n",
    "print(\"\\n\")\n",
    "print(\"Sentence: \")\n",
    "print(scary_string)\n",
    "print(\"Similarity to: \")\n",
    "print(\"happy \\t\\t sad \\t\\t scary \\t\\t disgust \\t\\t anger\")\n",
    "test(scary_string_words)\n",
    "print(\"\\n\")\n",
    "print(\"Sentence: \")\n",
    "print(angry_string)\n",
    "print(\"Similarity to: \")\n",
    "print(\"happy \\t\\t sad \\t\\t scary \\t\\t disgust \\t\\t anger\")\n",
    "test(angry_string_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results\n",
    "   While the reuslts are not exactly promising, 3 examples is in no way statitstically significant. Optimally I would like to establish a test dataset of sentences labeled by humans to get a more representative understanding of performance.\n",
    "## Outlook\n",
    " Once we establish a method to gauge the performance of an algorithm we can try to improve the performance (_if necesary_)\n",
    "  Potential improvements:\n",
    "  * Skipgram versus CBOW\n",
    "  * Larger training data-sets? \n",
    "  * Which words carry the most weight in a sentence? (Only use some combination of verbs, nouns, adjectives, ...)\n",
    "  * This was fun, but there are trained models out there that are most likely more efficient & are better trained. Use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
